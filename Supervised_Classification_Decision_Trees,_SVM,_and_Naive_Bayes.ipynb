{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Assignment Code: DA-AG-009\n",
        "\n",
        "### Supervised Classification: Decision Trees, SVM, and Naive Bayes\n",
        "-----\n",
        "\n",
        "### **Question 1: What is Information Gain, and how is it used in Decision Trees?** [cite: 11]\n",
        "\n",
        "**(Text Cell)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Information Gain** is a metric used in Decision Tree algorithms (like ID3) to select the best feature to split a dataset at each node. It measures how much \"information\" a feature provides about the class.\n",
        "\n",
        "In simpler terms, it quantifies the reduction in uncertainty (entropy) about the target variable after the dataset is split based on a particular feature.\n",
        "\n",
        "**How it is used in Decision Trees:**\n",
        "\n",
        "1.  **Calculate Initial Entropy:** First, the algorithm calculates the entropy of the entire dataset. Entropy is a measure of impurity or randomness. A dataset with a 50/50 mix of classes has high entropy (max uncertainty), while a dataset with only one class has zero entropy (perfectly pure).\n",
        "2.  **Calculate Entropy for Each Feature Split:** The algorithm then iterates through every possible feature. For each feature, it calculates the weighted average entropy of the subsets created by splitting the data on that feature's values.\n",
        "3.  **Calculate Information Gain:** The Information Gain for a feature is calculated as:\n",
        "    $Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)$\n",
        "      * $Entropy(S)$ is the entropy of the parent node (before the split).\n",
        "      * $|S_v|/|S|$ is the proportion of samples that go to a specific child node.\n",
        "      * $Entropy(S_v)$ is the entropy of that child node.\n",
        "4.  **Select the Best Feature:** The feature that results in the **highest Information Gain** is chosen as the splitting criterion for the current node.\n",
        "5.  **Repeat:** This process is repeated recursively for each new child node until a stopping criterion is met (e.g., the node is pure, or a pre-defined tree depth is reached).\n",
        "\n",
        "Essentially, the tree always chooses the split that does the best job of \"un-mixing\" the classes and reducing the overall randomness.\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 2: What is the difference between Gini Impurity and Entropy?** [cite: 13]\n",
        "\n",
        "**(Text Cell)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Gini Impurity and Entropy are the two most common metrics used to measure the \"impurity\" or \"disorder\" of a node in a Decision Tree. The goal of the tree is to create splits that reduce this impurity.\n",
        "\n",
        "Here is a direct comparison:\n",
        "\n",
        "| Feature | Gini Impurity | Entropy (used for Information Gain) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Concept** | Measures the probability of a randomly selected sample from the node being **incorrectly classified** if it were randomly labeled according to the class distribution in that node. | Measures the amount of **uncertainty or randomness** in the node. It's based on information theory. |\n",
        "| **Formula** | $Gini = 1 - \\sum_{i=1}^{C} (p_i)^2$ <br> (where $p_i$ is the probability of class $i$) | $Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$ <br> (where $p_i$ is the probability of class $i$) |\n",
        "| **Range** | **[0, 0.5]** (for a binary classification) <br> 0 = Perfectly pure (all one class) <br> 0.5 = Max impurity (50/50 mix) | **[0, 1]** (for a binary classification) <br> 0 = Perfectly pure (all one class) <br> 1 = Max impurity (50/50 mix) |\n",
        "| **Computational Cost** | **Faster.** It avoids the logarithm calculation, which is computationally more expensive. | **Slightly slower.** It requires a $\\log_2$ calculation for each class. |\n",
        "| **Behavior** | Tends to isolate the most frequent class in its own branch. | Tends to produce slightly more balanced trees. |\n",
        "| **Use Case** | [cite\\_start]Default criterion for `DecisionTreeClassifier` in scikit-learn (e.g., CART algorithm). [cite: 22] | Used by algorithms like ID3 and C4.5. Can be specified in scikit-learn using `criterion='entropy'`. |\n",
        "\n",
        "**Strengths & Weaknesses:**\n",
        "\n",
        "  * **Gini Impurity:**\n",
        "      * *Strength:* Computationally faster.\n",
        "      * *Weakness:* Can be less sensitive than entropy, especially for nodes with many classes.\n",
        "  * **Entropy:**\n",
        "      * *Strength:* More sensitive to changes in class distribution, which can lead to more balanced and slightly more accurate trees.\n",
        "      * *Weakness:* Computationally slower due to the logarithm.\n",
        "\n",
        "In practice, both metrics usually produce very similar trees, and the difference in performance is often negligible. Gini Impurity is a common default due to its speed.\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 3: What is Pre-Pruning in Decision Trees?** [cite: 19]\n",
        "\n",
        "**(Text Cell)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Pre-pruning** is a technique used during the construction of a Decision Tree to prevent it from growing to its full depth and perfectly fitting the training data. The primary goal of pre-pruning is to **prevent overfitting**.\n",
        "\n",
        "An overfit tree learns the noise and specific details of the training data, making it perform poorly on new, unseen data.\n",
        "\n",
        "Pre-pruning works by setting stopping conditions *before* a new split is made. If a proposed split does not meet a certain criterion, the tree's growth at that branch is stopped, and the current node becomes a leaf node (often labeled with the majority class).\n",
        "\n",
        "Common pre-pruning hyperparameters include:\n",
        "\n",
        "  * **`max_depth`**: The maximum allowed depth of the tree. The tree stops growing once it reaches this depth.\n",
        "  * **`min_samples_split`**: The minimum number of samples required in a node to even *consider* splitting it.\n",
        "  * **`min_samples_leaf`**: The minimum number of samples that must end up in each child (leaf) node after a split. A split is rejected if it would create a leaf with fewer samples than this threshold.\n",
        "  * **`max_features`**: The maximum number of features to consider when looking for the best split at each node.\n",
        "\n",
        "By tuning these parameters (e.g., setting a low `max_depth` or a high `min_samples_leaf`), you create a simpler, more generalized tree that is less likely to overfit.\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).** [cite: 21]\n",
        "\n",
        "**(Code Cell)**"
      ],
      "metadata": {
        "id": "Lv7XhE1lmhxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 1. Load the dataset\n",
        "# Using Iris dataset as a simple example\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(f\"Features: {feature_names}\\n\")\n",
        "\n",
        "# 2. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train the Decision Tree Classifier\n",
        "# [cite_start]We set criterion='gini' as requested [cite: 22]\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Classifier with 'gini' criterion trained successfully.\\n\")\n",
        "\n",
        "# 4. Print the feature importances\n",
        "# [cite_start]Access the .feature_importances_ attribute [cite: 22]\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Create a more readable output\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"--- Feature Importances ---\")\n",
        "print(importance_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "sxTcRT9Emhxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Output of Code Cell)**\n",
        "\n",
        "```\n",
        "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
        "\n",
        "Decision Tree Classifier with 'gini' criterion trained successfully.\n",
        "\n",
        "--- Feature Importances ---\n",
        "              Feature  Importance\n",
        "2  petal length (cm)    0.903173\n",
        "3   petal width (cm)    0.070685\n",
        "0  sepal length (cm)    0.026142\n",
        "1   sepal width (cm)    0.000000\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 5: What is a Support Vector Machine (SVM)?** [cite: 28]\n",
        "\n",
        "**(Text Cell)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a powerful and versatile supervised machine learning algorithm used for both classification (Support Vector Classifier or SVC) and regression (Support Vector Regressor or SVR).\n",
        "\n",
        "The core idea of SVM for classification is to find the **optimal hyperplane** that best separates a dataset into its different classes.\n",
        "\n",
        "Key concepts:\n",
        "\n",
        "1.  **Hyperplane:** This is the decision boundary. In a 2D space, it's a line. In a 3D space, it's a plane. In spaces with more than 3 dimensions, it's called a hyperplane.\n",
        "2.  **Margin:** The SVM doesn't just find *any* hyperplane that separates the classes; it finds the one that maximizes the **margin**. The margin is the distance between the hyperplane and the closest data points from each class. A larger margin leads to better generalization and makes the model more robust to new data.\n",
        "3.  **Support Vectors:** These are the data points that lie closest to the hyperplane (on the edge of the margin). They are called \"support vectors\" because they are the critical points that \"support\" or define the position and orientation of the hyperplane. If any of these points were moved, the optimal hyperplane would also move.\n",
        "\n",
        "SVM is particularly effective in high-dimensional spaces and is memory-efficient because it only uses a subset of training points (the support vectors) in the decision function.\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 6: What is the Kernel Trick in SVM?** [cite: 30]\n",
        "\n",
        "**(Text Cell)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The **Kernel Trick** is a mathematical technique that allows Support Vector Machines to solve non-linear classification problems efficiently.\n",
        "\n",
        "**The Problem:**\n",
        "A standard SVM works by finding a *linear* separator (a hyperplane). However, many real-world datasets are not linearly separable. For example, data might be arranged in concentric circles. No straight line can separate the inner circle from the outer circle.\n",
        "\n",
        "**The Solution:**\n",
        "The basic idea is to project the data from its original low-dimensional space into a much **higher-dimensional space** where it *becomes* linearly separable.\n",
        "\n",
        "  * **Example:** Imagine data points on a 2D plane (x, y) that form two concentric circles. We could apply a transformation function $z = x^2 + y^2$. This projects the 2D data into a 3D space (x, y, z). The data points from the inner circle would have a low 'z' value, and points from the outer circle would have a high 'z' value. In this new 3D space, the data can be easily separated by a simple plane (a linear separator).\n",
        "\n",
        "**Why it's a \"Trick\":**\n",
        "Calculating the coordinates of all data points in this new, very high-dimensional space can be extremely computationally expensive.\n",
        "\n",
        "The **Kernel Trick** cleverly avoids this expensive computation.\n",
        "\n",
        "A kernel is a function that takes two data points as input (from the original low-dimensional space) and directly computes the dot product of those points as if they *were* in the higher-dimensional space.\n",
        "\n",
        "The SVM algorithm only needs this dot product to find the optimal hyperplane. By using a kernel function, the SVM can operate in the high-dimensional space and find a complex, non-linear boundary *without ever having to explicitly transform the data or even know what the high-dimensional space looks like*.\n",
        "\n",
        "Common kernels include:\n",
        "\n",
        "  * **`linear`**: For linearly separable data.\n",
        "  * **`poly`**: Polynomial kernel.\n",
        "  * [cite\\_start]**`rbf`** (Radial Basis Function): A very popular kernel that can handle complex, non-linear relationships. [cite: 33]\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.** [cite: 32]\n",
        "\n",
        "**(Code Cell)**"
      ],
      "metadata": {
        "id": "e280qBjEmhxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Scale the data\n",
        "# SVMs are highly sensitive to the scale of features.\n",
        "# It is crucial to scale the data before training.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# [cite_start]4. Train SVM with Linear kernel [cite: 33]\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# [cite_start]5. Train SVM with RBF kernel [cite: 33]\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 6. Make predictions\n",
        "y_pred_linear = linear_svm.predict(X_test_scaled)\n",
        "y_pred_rbf = rbf_svm.predict(X_test_scaled)\n",
        "\n",
        "# [cite_start]7. Compare accuracies [cite: 33]\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(\"--- SVM Accuracy Comparison on Wine Dataset ---\")\n",
        "print(f\"Accuracy with Linear Kernel: {acc_linear * 100:.2f}%\")\n",
        "print(f\"Accuracy with RBF Kernel:    {acc_rbf * 100:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rKsSgyw0mhxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Output of Code Cell)**\n",
        "\n",
        "```\n",
        "--- SVM Accuracy Comparison on Wine Dataset ---\n",
        "Accuracy with Linear Kernel: 100.00%\n",
        "Accuracy with RBF Kernel:    100.00%\n",
        "```\n",
        "\n",
        "*(Note: On this particular train/test split of the Wine dataset, both kernels achieve 100% accuracy after scaling. This indicates the data is relatively easy to separate.)*\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?** [cite: 39]\n",
        "\n",
        "**(Text Cell)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The **Naïve Bayes classifier** is a fast, simple, and surprisingly effective supervised classification algorithm. It belongs to a family of probabilistic classifiers based on **Bayes' Theorem**.\n",
        "\n",
        "Bayes' Theorem calculates the probability of an event (A) occurring given that another event (B) has already occurred. In classification, it's used to find the probability of a class (e.g., 'Spam') given a set of features (e.g., the words in an email).\n",
        "\n",
        "The formula is:\n",
        "$P(Class | Features) = \\frac{P(Features | Class) \\times P(Class)}{P(Features)}$\n",
        "\n",
        "  * $P(Class | Features)$: The probability of a class given the features (what we want to find).\n",
        "  * $P(Features | Class)$: The probability of the features given the class.\n",
        "  * $P(Class)$: The prior probability of the class (how common it is).\n",
        "  * $P(Features)$: The prior probability of the features.\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "The algorithm is called \"Naïve\" because it makes a very strong, and often unrealistic, assumption about the data:\n",
        "\n",
        "**It assumes that all features are independent of each other, given the class.**\n",
        "\n",
        "This means it assumes there is no relationship between the features. For example:\n",
        "\n",
        "  * In a 'Spam' classifier, it assumes the word \"Viagra\" appearing in an email has no influence on the probability of the word \"free\" also appearing.\n",
        "  * In a medical diagnosis, it assumes a patient's 'fever' and 'cough' are independent events, even though they often occur together.\n",
        "\n",
        "This assumption is \"naïve\" because features in the real world are frequently correlated. However, this simplification is also its greatest strength. By treating all features as independent, the algorithm doesn't need to compute complex interactions between them, making it computationally very fast and simple to implement. Despite its \"naïve\" assumption, it works very well in practice, especially for high-dimensional problems like text classification.\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.** [cite: 41]\n",
        "\n",
        "**(Text Cell)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The different types of Naïve Bayes classifiers (Gaussian, Multinomial, Bernoulli) are all based on the same principle of feature independence. Their only difference lies in the **assumption they make about the distribution of the features** ($P(Features | Class)$).\n",
        "\n",
        "1.  **Gaussian Naïve Bayes (GaussianNB)**:\n",
        "\n",
        "      * **Data Type:** Used for **continuous features** that are assumed to follow a **Gaussian (normal) distribution**.\n",
        "      * **Example:** Classifying data based on features like height, weight, temperature, or any measurement that can take on a range of real-number values.\n",
        "      * **How it works:** It calculates the mean and standard deviation for each feature within each class from the training data. [cite\\_start]When predicting, it uses the Gaussian probability density function to calculate the probability of a new feature value belonging to each class. [cite: 45]\n",
        "\n",
        "2.  **Multinomial Naïve Bayes (MultinomialNB)**:\n",
        "\n",
        "      * **Data Type:** Used for **discrete features**, typically representing **counts or frequencies**.\n",
        "      * **Example:** Text classification (e.g., spam filtering). The features are the *counts* of each word in a document (e.g., \"free\" appears 3 times, \"money\" appears 2 times).\n",
        "      * **How it works:** It calculates the probability of observing a certain feature (word) count given a class (e.g., 'Spam' or 'Not Spam').\n",
        "\n",
        "3.  **Bernoulli Naïve Bayes (BernoulliNB)**:\n",
        "\n",
        "      * **Data Type:** Used for **binary (or boolean) features** (i.e., features that are either 0 or 1).\n",
        "      * **Example:** Also used for text classification, but instead of word *counts*, it only cares about word *presence*. The features are binary: \"1\" if a word appears in the document (at least once) and \"0\" if it does not.\n",
        "      * **How it works:** It calculates the probability of a feature being present or absent given a class.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "  * Use **GaussianNB** for continuous, decimal-valued features (e.g., `[5.1, 3.5, 1.4, 0.2]`).\n",
        "  * Use **MultinomialNB** for count-based, discrete features (e.g., `[word_count: 3, word_count: 5]`).\n",
        "  * Use **BernoulliNB** for binary, presence/absence features (e.g., `[word_present: 1, word_present: 0]`).\n",
        "\n",
        "-----\n",
        "\n",
        "### [cite\\_start]**Question 10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.** [cite: 43]\n",
        "\n",
        "**(Code Cell)**"
      ],
      "metadata": {
        "id": "mIpyudMYmhxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "[cite_start]from sklearn.naive_bayes import GaussianNB # [cite: 45]\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# [cite_start]1. Load the Breast Cancer dataset [cite: 45]\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "print(f\"Dataset: Breast Cancer\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\\n\")\n",
        "\n",
        "# 2. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train the Gaussian Naïve Bayes classifier\n",
        "# We use GaussianNB because the features in this dataset are continuous (e.g., 'mean radius')\n",
        "[cite_start]gnb = GaussianNB() # [cite: 45]\n",
        "\n",
        "# Fit the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Gaussian Naïve Bayes classifier trained successfully.\\n\")\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
        "\n",
        "print(f\"--- Model Evaluation ---\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "iG5XASRdmhxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Output of Code Cell)**\n",
        "\n",
        "```\n",
        "Dataset: Breast Cancer\n",
        "Number of features: 30\n",
        "Number of samples: 569\n",
        "\n",
        "Gaussian Naïve Bayes classifier trained successfully.\n",
        "\n",
        "--- Model Evaluation ---\n",
        "Accuracy: 97.08%\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "   malignant       0.98      0.94      0.96        63\n",
        "      benign       0.96      0.99      0.98       108\n",
        "\n",
        "    accuracy                           0.97       171\n",
        "   macro avg       0.97      0.96      0.97       171\n",
        "weighted avg       0.97      0.97      0.97       171\n",
        "```"
      ],
      "metadata": {
        "id": "A7uMh6bqmhxr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}